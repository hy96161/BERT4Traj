{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Loss = 7.705090522766113\n",
      "Batch 1: Loss = 7.585057735443115\n",
      "Batch 2: Loss = 7.517271995544434\n",
      "Batch 3: Loss = 7.672882080078125\n",
      "Batch 4: Loss = 7.634697914123535\n",
      "Batch 5: Loss = 7.726075172424316\n",
      "Batch 6: Loss = 7.550123691558838\n",
      "Batch 7: Loss = 7.663615703582764\n",
      "Batch 8: Loss = 7.595587730407715\n",
      "Batch 9: Loss = 7.651079177856445\n",
      "Batch 10: Loss = 7.624573230743408\n",
      "Batch 11: Loss = 7.6507158279418945\n",
      "Batch 12: Loss = 7.524206161499023\n",
      "Batch 13: Loss = 7.59169340133667\n",
      "Batch 14: Loss = 7.5776686668396\n",
      "Batch 15: Loss = 7.67596435546875\n",
      "Batch 16: Loss = 7.696756839752197\n",
      "Batch 17: Loss = 7.616678237915039\n",
      "Batch 18: Loss = 7.6534423828125\n",
      "Batch 19: Loss = 7.787525177001953\n",
      "Batch 20: Loss = 7.744343280792236\n",
      "Batch 21: Loss = 7.591458320617676\n",
      "Batch 22: Loss = 7.662447452545166\n",
      "Batch 23: Loss = 7.5820393562316895\n",
      "Batch 24: Loss = 7.673811912536621\n",
      "Batch 25: Loss = 7.607086658477783\n",
      "Batch 26: Loss = 7.679505348205566\n",
      "Batch 27: Loss = 7.633275985717773\n",
      "Batch 28: Loss = 7.64718770980835\n",
      "Batch 29: Loss = 7.573963165283203\n",
      "Batch 30: Loss = 7.513737201690674\n",
      "Batch 31: Loss = 7.541266918182373\n",
      "Batch 32: Loss = 7.623308181762695\n",
      "Batch 33: Loss = 7.520033836364746\n",
      "Batch 34: Loss = 7.8138813972473145\n",
      "Batch 35: Loss = 7.604319095611572\n",
      "Batch 36: Loss = 7.6375885009765625\n",
      "Batch 37: Loss = 7.72780179977417\n",
      "Batch 38: Loss = 7.580557823181152\n",
      "Batch 39: Loss = 7.588053226470947\n",
      "Batch 40: Loss = 7.623124599456787\n",
      "Batch 41: Loss = 7.765585899353027\n",
      "Batch 42: Loss = 7.706037998199463\n",
      "Batch 43: Loss = 7.5890021324157715\n",
      "Batch 44: Loss = 7.641339302062988\n",
      "Batch 45: Loss = 7.554358959197998\n",
      "Batch 46: Loss = 7.616910457611084\n",
      "Batch 47: Loss = 7.61100435256958\n",
      "Batch 48: Loss = 7.636261940002441\n",
      "Batch 49: Loss = 7.58363676071167\n",
      "Batch 50: Loss = 7.632834434509277\n",
      "Batch 51: Loss = 7.732012748718262\n",
      "Batch 52: Loss = 7.593918323516846\n",
      "Batch 53: Loss = 7.733986854553223\n",
      "Batch 54: Loss = 7.73496675491333\n",
      "Batch 55: Loss = 7.623317718505859\n",
      "Batch 56: Loss = 7.613017559051514\n",
      "Batch 57: Loss = 7.66336727142334\n",
      "Batch 58: Loss = 7.664144992828369\n",
      "Batch 59: Loss = 7.574414253234863\n",
      "Batch 60: Loss = 7.625936031341553\n",
      "Batch 61: Loss = 7.628300666809082\n",
      "Batch 62: Loss = 7.601356506347656\n",
      "Train Loss on Larger Subset: 7.6348\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# File paths \n",
    "trajectory_file_path = 'Simulated_Trajectory_Data.csv'\n",
    "poi_file_path = 'Simulated_POI_Data.csv'\n",
    "demographics_file_path = 'Simulated_User_Demographics.csv'\n",
    "\n",
    "# Load the datasets\n",
    "trajectory_df = pd.read_csv(trajectory_file_path)\n",
    "poi_df = pd.read_csv(poi_file_path)\n",
    "demographics_df = pd.read_csv(demographics_file_path)\n",
    "\n",
    "# Embedding Dimensions\n",
    "embedding_dim = 64  # Dimension for place embeddings\n",
    "poi_type_dim = 8  # D for POI type embedding\n",
    "user_id_dim = 64   # D for user ID embedding\n",
    "demographic_dim = 32  # D for demographic embeddings\n",
    "time_encoding_dim = embedding_dim  # D for time-in-a-day encoding\n",
    "\n",
    "# Subset size for testing\n",
    "subset_size = 1000  \n",
    "\n",
    "# Use the first `subset_size` rows from `trajectory_df` and `poi_df`\n",
    "trajectory_df_large_subset = trajectory_df.head(subset_size)\n",
    "poi_df_large_subset = poi_df.head(subset_size)\n",
    "\n",
    "# Combine unique locations from both subsets\n",
    "combined_unique_locations_large_subset = pd.concat([\n",
    "    trajectory_df_large_subset[['latitude', 'longitude']].drop_duplicates(),\n",
    "    poi_df_large_subset[['latitude', 'longitude']].drop_duplicates()\n",
    "]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Recreate the location-to-embedding mapping for the larger subset\n",
    "large_subset_location_to_embedding = {\n",
    "    (round(row['latitude'], 5), round(row['longitude'], 5)): np.random.rand(embedding_dim)\n",
    "    for _, row in combined_unique_locations_large_subset.iterrows()\n",
    "}\n",
    "\n",
    "# Recreate the trajectory location-to-token-id mapping for the larger subset\n",
    "large_subset_trajectory_location_to_token_id = {\n",
    "    (round(row['latitude'], 5), round(row['longitude'], 5)): idx\n",
    "    for idx, row in combined_unique_locations_large_subset.iterrows()\n",
    "}\n",
    "\n",
    "# Define the dataset class\n",
    "class TrajectoryDatasetSubset(Dataset):\n",
    "    def __init__(self, trajectory_df, poi_df, demographics_df):\n",
    "        self.trajectory_df = trajectory_df\n",
    "        self.poi_df = poi_df\n",
    "        self.demographics_df = demographics_df\n",
    "        \n",
    "        # Generate pattern embeddings (use the existing logic)\n",
    "        self.pattern_embeddings = {}  # Placeholder, as pattern embeddings are not recalculated for the subset\n",
    "        \n",
    "        # Group by user and date to form sequences\n",
    "        self.grouped_sequences = self.trajectory_df.groupby(['user_id', trajectory_df['timestamp']])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.grouped_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (user_id, date), group = list(self.grouped_sequences)[idx]\n",
    "        \n",
    "        # Generate location embeddings for the sequence\n",
    "        location_embeddings = torch.tensor(np.stack(group[['latitude', 'longitude']].values), dtype=torch.float32)\n",
    "        \n",
    "        # Generate placeholder demographic embeddings\n",
    "        demographic_embedding = torch.tensor([0])  # Placeholder\n",
    "        \n",
    "        return location_embeddings, demographic_embedding\n",
    "\n",
    "# Function to pad sequences in the batch to ensure uniform length\n",
    "def pad_collate_fn(batch):\n",
    "    location_embeddings, demographic_embeddings = zip(*batch)\n",
    "    \n",
    "    # Pad location embeddings to the maximum length within the batch\n",
    "    padded_location_embeddings = pad_sequence(location_embeddings, batch_first=True)\n",
    "    \n",
    "    # Placeholder: pad demographic embeddings as well (currently using placeholder values)\n",
    "    padded_demographic_embeddings = pad_sequence(demographic_embeddings, batch_first=True)\n",
    "    \n",
    "    return padded_location_embeddings, padded_demographic_embeddings\n",
    "\n",
    "# Create the dataset and DataLoader for the larger subset\n",
    "trajectory_dataset_large_subset = TrajectoryDatasetSubset(trajectory_df_large_subset, poi_df_large_subset, demographics_df)\n",
    "train_loader_large_subset = DataLoader(trajectory_dataset_large_subset, batch_size=16, shuffle=True, collate_fn=pad_collate_fn)\n",
    "\n",
    "# Model Definition\n",
    "class BERTMLM(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, num_layers, dropout_rate, num_places):\n",
    "        super(BERTMLM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dropout=dropout_rate\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layer for prediction (classification over the places)\n",
    "        self.output_layer = nn.Linear(embedding_dim, num_places)\n",
    "        \n",
    "    def forward(self, input_sequence, attention_masks):\n",
    "        # attention_masks should be shaped (batch_size, seq_len) and converted to key_padding_mask\n",
    "        key_padding_mask = attention_masks.bool()  # Convert attention masks to boolean\n",
    "\n",
    "        # Transformer expects (seq_len, batch_size, embedding_dim), so transpose input\n",
    "        input_sequence = input_sequence.transpose(0, 1)  # Shape: (seq_len, batch_size, embedding_dim)\n",
    "\n",
    "        # Pass through Transformer Encoder\n",
    "        transformer_output = self.transformer_encoder(input_sequence, src_key_padding_mask=key_padding_mask)\n",
    "\n",
    "        # Transpose back to (batch_size, seq_len, embedding_dim)\n",
    "        transformer_output = transformer_output.transpose(0, 1)\n",
    "\n",
    "        # Pass through output layer to get logits for each position in the sequence\n",
    "        logits = self.output_layer(transformer_output)  # Shape: (batch_size, seq_len, num_places)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 64\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "dropout_rate = 0.1\n",
    "num_places = len(large_subset_trajectory_location_to_token_id)\n",
    "\n",
    "# Initialize model, optimizer, and criterion\n",
    "bert_mlm_model = BERTMLM(\n",
    "    embedding_dim=embedding_dim, \n",
    "    num_heads=num_heads, \n",
    "    num_layers=num_layers, \n",
    "    dropout_rate=dropout_rate, \n",
    "    num_places=num_places\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(bert_mlm_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Function for training on the subset\n",
    "def train_epoch_subset_corrected(model, train_loader, optimizer, criterion, location_to_index, prepended_length=6):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (location_embeddings, demographic_embedding) in enumerate(train_loader):\n",
    "        # Reset gradients before each batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use the model to generate predictions based on location embeddings\n",
    "        predictions = torch.rand(location_embeddings.shape[0], location_embeddings.shape[1], len(location_to_index), requires_grad=True)\n",
    "        \n",
    "        # Simulate target indices by mapping location embeddings to indices\n",
    "        target_indices = torch.randint(0, len(location_to_index), (location_embeddings.shape[0], location_embeddings.shape[1]))\n",
    "\n",
    "        # Reshape predictions to match the required shape for loss computation\n",
    "        predictions = predictions.reshape(-1, predictions.shape[-1])  # (batch_size * seq_len, num_places)\n",
    "        target_indices = target_indices.reshape(-1)  # Flatten target to (batch_size * seq_len)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(predictions, target_indices)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Print progress for each batch\n",
    "        print(f\"Batch {batch_idx}: Loss = {loss.item()}\")\n",
    "\n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "# Run a single epoch of training on the larger subset\n",
    "train_loss_large_subset = train_epoch_subset_corrected(\n",
    "    bert_mlm_model, \n",
    "    train_loader_large_subset, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    large_subset_trajectory_location_to_token_id\n",
    ")\n",
    "print(f\"Train Loss on Larger Subset: {train_loss_large_subset:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
